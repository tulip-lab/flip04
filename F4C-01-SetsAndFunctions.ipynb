{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (04): Learning Theory (I)\n",
    "**(Module 03: Convex Optimization)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sets And Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will study the unconstrained problem\n",
    "\n",
    "[1]\n",
    "\\begin{equation*}\n",
    " \\minimize\\ f(\\vct{x}),\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\vct{x}\\in \\R^n$. **Optimality conditions** aim to identify properties that potential minimizers need to satisfy in relation to $f(\\vct{x})$. We will review the well known local optimality conditions for differentiable functions from calculus. We then introduce convex functions and discuss some of their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to [1] come in different flavours, as in the following definition.\n",
    "\n",
    "**Definition.** A point $\\vct{x}^*\\in \\R^n$ is a\n",
    "\n",
    " * **global minimizer** of [1] if for all $\\vct{x}\\in U$, $f(\\vct{x}^*)\\leq f(\\vct{x})$;\n",
    " * a **local minimizer**, if there is an open neighbourhood $U$ of $\\vct{x}$ such that $f(\\vct{x}^*)\\leq f(\\vct{x})$ for all $\\vct{x}\\in U$;\n",
    " * a **strict local minimizer**, if there is an open neighbourhood $U$ of $\\vct{x}$ such that $f(\\vct{x}^*)<f(\\vct{x})$ for all $\\vct{x}\\in U$;\n",
    " * an **isolated minimizer** if there is an open neighbourhood $U$ of $\\vct{x}^*$ such that $\\vct{x}^*$ is the only local minimizer in $U$.\n",
    "\n",
    "Without any further assumptions on $f$, finding a minimizer is a hopeless task: we simply can't examine the function at *all* points in $\\R^n$. \n",
    "The situation becomes more tractable if we assume some *smoothness* conditions. Recall that $C^k(U)$ denotes the set of functions that are $k$ times continuously differentiable on some set $U$. The following **first-order necessary condition** for optimality is well known. We write $\\nabla f(\\vct{x})$ for the gradient of $f$ at $\\vct{x}$, i.e., the vector \n",
    "\n",
    "\\begin{equation*}\n",
    " \\nabla f(\\vct{x}) = \\left(\\frac{\\partial f}{\\partial x_1}(\\vct{x}),\\dots,\\frac{\\partial f}{\\partial x_n}(\\vct{x})\\right)^{\\trans}\n",
    "\\end{equation*}\n",
    "\n",
    "**Theorem.** Let $\\vct{x}^*$ be a local minimizer of $f$ and assume that $f\\in C^1(U)$ for a neighbourhood of $U$ of $\\vct{x}^*$. Then $\\nabla f(\\vct{x}^*) = \\zerovct$. \n",
    "\n",
    "There are simple examples that show that this is not a sufficient condition: maxima and saddle points will also have a vanishing gradient. If we have access to *second-order information*, in form of the second derivative, or Hessian, of $f$, then we can say more. Recall that the Hessian of $f$ at $\\vct{x}$, $\\nabla^2f(\\vct{x})$, is the $n\\times n$ symmetric matrix given by the second derivatives,\n",
    "\n",
    "\\begin{equation*}\n",
    " \\nabla^2f(\\vct{x}) = \\left(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\right)_{1\\leq i,j\\leq n}.\n",
    "\\end{equation*}\n",
    "\n",
    "In the one-variable case we know that if $x^*$ is a local minimizer of $f\\in C^2([a,b])$, then $f'(x^*)=0$ *and* $f''(x^*)\\geq 0$. Moreover, the conditions $f'(x^*)=0$ *and* $f''(x^*)>0$ guarantee that we have a local minimizer. These conditions generalise to higher dimension, but first we need to know what $f''(x)>0$ means when we have more than one variable.\n",
    "\n",
    "Recall that a matrix $\\mtx{A}$ is **positive semidefinite**, written $\\mtx{A}\\succeq \\zerovct$, if for every $\\vct{x}\\in \\R^n$, $\\vct{x}^{\\top}\\mtx{A}\\vct{x}\\geq 0$, and **positive definite**, written $\\mtx{A}\\succ \\zerovct$, if $\\vct{x}^{\\top}\\mtx{A}\\vct{x}>0$. The property that the Hessian matrix is positive semidefinite is a multivariate generalization of the property that the second derivative is nonnegative. The known conditions for a minimizer involving the second derivative generalise accordingly.\n",
    "\n",
    "**Theorem.** Let $f\\in C^2(U)$ for some open set $U$ and $\\vct{x}^*\\in U$. \n",
    " If $\\vct{x}^*$ is a local minimizer, then $\\nabla f(\\vct{x}^*)=0$ and $\\nabla^2f(\\vct{x}^*)$  is positive semidefinite. Conversely, if $\\nabla f(\\vct{x}^*)=\\zerovct$ and $\\nabla^2f(\\vct{x}^*)$ is positive definite, then $\\vct{x}^*$ is a strict local minimizer. \n",
    "\n",
    "Unfortunately, the above criteria are not able to identify global minimizers, as differentiability is a local property. If, however, the function is **convex**, then we can say a lot more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex sets and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now come to the central notion of this course. \n",
    "\n",
    "**Definition.** A set $C\\subseteq \\R^n$ is **convex** if for all $\\vct{x},\\vct{y}\\in C$ and $\\lambda \\in [0,1]$, the line $\\lambda \\vct{x}+(1-\\lambda)\\vct{y}\\in C$. A **convex body** is a convex set that is closed and bounded.\n",
    "\n",
    "**Definition.**\n",
    "Let $S\\subseteq \\R^n$. A function $f\\colon S\\to \\R$ is called **convex** if $S$ is convex and for all $\\vct{x},\\vct{y}\\in \\R^n$ and $\\lambda\\in [0,1]$,\n",
    "\n",
    "\\begin{equation*}\n",
    " f(\\lambda \\vct{x}+(1-\\lambda)\\vct{y})\\leq \\lambda f(\\vct{x})+(1-\\lambda)f(\\vct{y}).\n",
    "\\end{equation*}\n",
    "\n",
    "The function $f$ is called **strictly convex** if\n",
    "\n",
    "\\begin{equation*}\n",
    " f(\\lambda \\vct{x}+(1-\\lambda)\\vct{y})< \\lambda f(\\vct{x})+(1-\\lambda)f(\\vct{y}).\n",
    "\\end{equation*}\n",
    "\n",
    "A function $f$ is called **concave**, if $-f$ is convex. \n",
    "\n",
    "The next figure illustrates how a convex set in $\\R^2$ and a function of one variable looks like. The graph of the function lies below any line connecting two points on it.\n",
    "\n",
    "Convex function have pleasant properties, while at the same time covering many of the functions that arise in applications. Perhaps the most important property is that local minima are global minima.\n",
    "\n",
    "**Theorem.** Let $f\\colon \\R^n\\to \\R$ be a convex function. Then any local minimizer of $f$ is a global minimizer.\n",
    "\n",
    "**Proof.** Let $\\vct{x}^*$ be a local minimizer and assume that it is not a global minimizer. Then there exists a vector $\\vct{y}\\in \\R^n$ such that $f(\\vct{y})<f(\\vct{x}^*)$. Since $f$ is convex, for any $\\lambda\\in [0,1]$ and $\\vct{x}=\\lambda \\vct{y}+(1-\\lambda)\\vct{x}^*$ we have\n",
    "\n",
    " \\begin{equation*}\n",
    "  f(\\vct{x}) \\leq \\lambda f(\\vct{y})+(1-\\lambda) f(\\vct{x}^*) < \\lambda f(\\vct{x}^*)+(1-\\lambda)f(\\vct{x}^*) = f(\\vct{x}^*).\n",
    " \\end{equation*}\n",
    "\n",
    "This holds for all $\\vct{x}$ on the line segment connecting $\\vct{y}$ and $\\vct{x}^*$. Since every open neighbourhood $U$ of $\\vct{x}^*$ contains a bit of this line segment, this means that every open neighbourhood $U$ of $\\vct{x}^*$ contains an $\\vct{x}\\neq \\vct{x}^*$ such that $f(\\vct{x})\\leq f(\\vct{x}^*)$, in contradiction to the assumption that $\\vct{x}^*$ is a local minimizer. It follows that $\\vct{x}^*$ has to be a global minimizer.\n",
    "\n",
    "** Remark.** Note that in the above theorem we made no assumptions about the differentiability of the function $f$! In fact, while a convex function is always *continuous*, it need not be differentiable. The function $f(x) = |x|$ is a typical example: it is convex, but not differentiable at $x=0$.\n",
    "\n",
    "**Example.** Affine functions $f(\\vct{x})=\\ip{\\vct{x}}{\\vct{a}}+b$ and the exponential function $e^x$ are examples of convex functions. \n",
    "\n",
    "**Example.** In optimization we will often work with functions of matrices, where an $m\\times n$ matrix is considered as a vector in $\\R^{m\\times n}\\cong \\R^{mn}$. If the matrix is symmetric, that is, if $\\mtx{A}^{\\trans}=\\mtx{A}$, then we only care about the upper diagonal entries, and we consider the space $\\mathcal{S}^n$ of symmetric matrices as a vector space of dimension $n(n+1)/2$ (the number of entries on and above the main diagonal). Important functions on symmetric matrices that are convex are the operator norm $\\norm{\\mtx{A}}_2$, defined as\n",
    " \\begin{equation*}\n",
    "  \\norm{\\mtx{A}}_2 := \\max_{\\vct{x}\\colon \\norm{\\vct{x}}\\leq 1} \\frac{\\norm{\\mtx{A}\\vct{x}}_2}{\\norm{\\vct{x}}_2},\n",
    " \\end{equation*}\n",
    "or the function $\\log \\det(\\mtx{X})$, defined on the set of *positive semidefinite* symmetric matrices $\\mathcal{S}_+^n$.\n",
    "\n",
    "There are useful ways of characterising convexity using differentiability.\n",
    "\n",
    "**Theorem.**\n",
    "* Let $f\\in C^1(\\R^n)$. Then $f$ is convex if and only if for all $\\vct{x}, \\vct{y}\\in \\R^n$,\n",
    "\n",
    " \\begin{equation*}\n",
    "  f(\\vct{y})\\geq f(\\vct{x})+\\nabla f(\\vct{x})^{\\trans} (\\vct{y}-\\vct{x}).\n",
    " \\end{equation*}\n",
    "\n",
    "* Let $f\\in C^2(\\R^n)$. Then $f$ is convex if and only if $\\nabla^2 f(\\vct{x})$ is positive semidefinite. If $\\nabla^2f(\\vct{x})$ is positive definite, then $f$ is strictly convex.\n",
    "\n",
    "**Example.** Consider a quadratic function of the form\n",
    " \n",
    " \\begin{equation*}\n",
    "f(\\vct{x}) = \\frac{1}{2}\\vct{x}^{\\trans}\\mtx{A}\\vct{x}+\\vct{b}^{\\trans}\\vct{x}+c, \n",
    " \\end{equation*}\n",
    " \n",
    " where $\\mtx{A}\\in \\R^{n\\times n}$ is symmetric. Writing out the product, we get\n",
    " \n",
    " \\begin{equation*}\n",
    "   \\mtx{x}^{T}\\mtx{A}\\vct{x} = \\begin{pmatrix} x_1 & \\cdots & x_n\n",
    "   \\end{pmatrix}\n",
    "   \\begin{pmatrix}\n",
    "   a_{11} & \\cdots & a_{1n}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   a_{n1} & \\cdots & a_{nn}\n",
    "   \\end{pmatrix}\n",
    "   \\begin{pmatrix}\n",
    "   x_1\\\\ \\vdots \\\\ x_n\n",
    "   \\end{pmatrix} = \\begin{pmatrix}\n",
    "   x_1 & \\cdots & x_n\n",
    "   \\end{pmatrix}\n",
    "   \\begin{pmatrix}\n",
    "   a_{11}x_1+\\cdots+a_{1n}x_n\\\\\n",
    "   \\vdots \\\\\n",
    "   a_{n1}x_1+\\cdots+a_{nn}x_n\n",
    "   \\end{pmatrix} = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j.\n",
    " \\end{equation*} \n",
    " \n",
    " Because $\\mtx{A}$ is symmetric, we have $a_{ij}=a_{ji}$, and the above product simplifies to\n",
    " \n",
    " \\begin{equation*}\n",
    " \\vct{x}^{T}\\mtx{A}\\vct{x} = \\sum_{i=1}^n a_{ii} x_i^2 + 2\\sum_{1\\leq i<j\\leq n} a_{ij}x_i x_j.\n",
    " \\end{equation*}\n",
    " \n",
    " This is a quadratic function, because it involves products of the $x_i$. The gradient and the Hessian of $f(\\vct{x})$ are found by computing the first and second order partial derivatives of $f$:\n",
    " \n",
    " \\begin{equation*}\n",
    " \\frac{\\partial f}{\\partial x_i} = \\sum_{j=1}^n a_{ij} x_j + b_i, \\quad \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = a_{ij}.\n",
    " \\end{equation*}\n",
    " \n",
    " In summary, we have\n",
    " \\begin{equation*}\n",
    "   \\nabla f(\\vct{x}) = \\mtx{A}\\vct{x} + \\vct{b}, \\quad \\nabla^2f(\\vct{x}) = \\mtx{A}.\n",
    " \\end{equation*}\n",
    " \n",
    " Using the previous theorem, we see that $f$ is convex **if and only if** $\\mtx{A}$ is positive semidefinite. A typical example for such a function is the least squares function encountered in Lecture 1,\n",
    " \n",
    " \\begin{equation*}\n",
    "   f(\\vct{x}) = \\|\\mtx{X}\\vct{\\beta}-\\vct{y}\\|^2 = (\\mtx{X}\\vct{\\beta}-\\vct{y})^{T}(\\mtx{X}\\vct{\\beta}-\\vct{y}) = \\vct{\\beta}^{T}\\mtx{X}^{T}\\mtx{X}\\vct{\\beta} -2\\vct{y}^{T}\\mtx{X}\\vct{\\beta}+\\vct{y}^T\\vct{y}.\n",
    " \\end{equation*}\n",
    " \n",
    " The matrix $\\mtx{A}=\\mtx{X}^T\\mtx{X}$ is always symmetric and positive semidefinite (why?) so that the function $f$ is convex. In the example below, we plot the mean squared error of a random least squares problem (see Lecture 1 for an explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient way to visualise a function $f\\colon \\R^2\\to \\R$ is through **contour plots**. A **level set** of the function $f$ is a set of the form\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\{\\vct{x}\\mid f(\\vct{x}) = c\\},\n",
    "\\end{equation*}\n",
    "\n",
    "where $c$ is the **level**. Each such level set is a curve in $\\R^2$, and a contour plot is a plot of a collection of such curves for various $c$. If one colours the areas between adjacent curves, one gets a plot as in the following figure. A *convex function* is has the property that there is only one *sink* in the contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Create random data: we use the randn function\n",
    "X = np.random.randn(3,2)\n",
    "y = np.random.randn(3)\n",
    "\n",
    "# Solve least squares problem minimize \\|X\\beta-y\\|^2\n",
    "# the index 0 says that we get the first component of the solution\n",
    "# (the function lstsq give more output than just the beta vector)\n",
    "beta = la.lstsq(X,y)[0]\n",
    "\n",
    "# Create function and plot the contours\n",
    "def f(a,b):\n",
    "    return sum((a*X[:,0]+b*X[:,1]-y)**2)\n",
    "\n",
    "# Find the \"right\" boundaries around the minimum\n",
    "xx = np.linspace(beta[0]-8,beta[0]+8,100)\n",
    "yy = np.linspace(beta[1]-8,beta[1]+8,100)\n",
    "\n",
    "# A mesh grid gives a pair of matrix XX and YY,\n",
    "# such that for each (i,j), the pairs (XX[i,j],YY[i,j])\n",
    "# cover all the points on the square defined by xx and yy\n",
    "XX, YY = np.meshgrid(xx,yy)\n",
    "\n",
    "# Compute the Z values corresponding to the meshgrid\n",
    "Z = np.zeros(XX.shape)\n",
    "for i in range(Z.shape[0]):\n",
    "    for j in range(Z.shape[1]):\n",
    "        Z[i,j] = f(XX[i,j],YY[i,j])\n",
    "\n",
    "# Before plotting the contour map, choose a nice colormap\n",
    "cmap = plt.cm.get_cmap(\"coolwarm\")\n",
    "plt.contourf(XX,YY,Z, cmap = cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
